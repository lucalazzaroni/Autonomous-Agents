{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake on Gym\n",
    "Gym allows you to exploit pre-built environments, including the Frozen Lake one. If ```is_slippery``` is set to ```True```, the agent will have a 1/3 probability to move left, up or down, regardless of the action chosen.\n",
    "\n",
    "You can render the environment by passing to the ```gym.make``` function the ```render_mode='human'``` argument. If several episodes need to be run, you can avoid rendering to speed up operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent\n",
    "We start with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode='human')\n",
    "\n",
    "num_episodes = 10\n",
    "goals_reached = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    terminated = False\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}')\n",
    "\n",
    "    while not terminated:\n",
    "        \n",
    "        # sample a random action from the list of available actions\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # perform this action on the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if terminated and obs == 15:\n",
    "            '''\n",
    "            The observation is a value representing the agent's current position as\n",
    "            current_row * nrows + current_col (where both the row and col start at 0).\n",
    "            For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "            '''\n",
    "            \n",
    "            print('Success!')\n",
    "            goals_reached += 1\n",
    "            \n",
    "# end this instance of the environment\n",
    "env.close()\n",
    "print(f'\\nSuccess rate:\\t{goals_reached / num_episodes * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyboard controlled Agent\n",
    "Now let's try ourselves to move the agent and see if we can do better than the random agent.\n",
    "\n",
    "WASD keyboard keys should be used, remember to click buttons inside the pygame window, not here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode='human')\n",
    "\n",
    "num_episodes = 10\n",
    "goals_reached = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    terminated = False\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}')\n",
    "\n",
    "    final_obs = env.simulate()     \n",
    "        \n",
    "    if final_obs == 15:\n",
    "        '''\n",
    "        The observation is a value representing the agent's current position as\n",
    "        current_row * nrows + current_col (where both the row and col start at 0).\n",
    "        For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "        '''\n",
    "        print('Success!')\n",
    "        goals_reached += 1\n",
    "            \n",
    "# end this instance of the environment\n",
    "env.close()\n",
    "print(f'\\nSuccess rate:\\t{goals_reached / num_episodes * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic Agent\n",
    "Next step is trying to develop a heuristic algorithm to solve the Frozen Lake environment. Here one is proposed, but you can also think about another one on your own üòÄ\n",
    "\n",
    "In the Frozen Lake environment the observation is calculated as follows: ```current_row * nrows + current_col```.\n",
    "\n",
    "We can therefore build a table that represents the observation value for each possible cell:\n",
    "```\n",
    "0   1   2   3\n",
    "4   5   6   7\n",
    "8   9   10  11\n",
    "12  13  14  15\n",
    "```\n",
    "Where:\n",
    "\n",
    "Starting point: ```0```\n",
    "\n",
    "Goal: ```15```\n",
    "\n",
    "Holes: ```5, 7, 11, 12```\n",
    "\n",
    "We can create a lookup table that maps each possible observation to an action that let the Agent avoid the holes as much as possible:\n",
    "```\n",
    "‚ñ∂   ‚ñ∂   ‚ñº   ‚óÄ\n",
    "‚ñº   ‚úñ   ‚ñº   ‚úñ\n",
    "‚ñ∂   ‚ñº   ‚ñº   ‚úñ\n",
    "‚úñ   ‚ñ∂   ‚ñ∂   ‚óè      \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, render_mode='human')\n",
    "\n",
    "actions = [2, 2, 1, 0, \n",
    "           1, -1, 1, -1, \n",
    "           2, 1, 1, -1, \n",
    "           -1, 2, 2, -1]\n",
    "\n",
    "num_episodes = 10\n",
    "goals_reached = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    terminated = False\n",
    "    obs = actions[0]\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}')\n",
    "\n",
    "    while not terminated:\n",
    "        \n",
    "        # choose action basing on current observation\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # perform this action on the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if terminated and obs == 15:\n",
    "            '''\n",
    "            The observation is a value representing the agent's current position as\n",
    "            current_row * nrows + current_col (where both the row and col start at 0).\n",
    "            For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "            '''\n",
    "            \n",
    "            print('Success!')\n",
    "            goals_reached += 1\n",
    "            \n",
    "# end this instance of the environment\n",
    "env.close()\n",
    "print(f'\\nSuccess rate:\\t{goals_reached / num_episodes * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to change the lookup table and run for 100 episodes to see if you are able to achieve better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's increase the difficulty!\n",
    "What if we increase the map size to 8x8? Are you able to build another heuristic agent?\n",
    "```\n",
    "‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†\n",
    "‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†\n",
    "‚ñ†   ‚ñ†   ‚ñ†       ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†  \n",
    "‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†       ‚ñ†   ‚ñ†\n",
    "‚ñ†   ‚ñ†   ‚ñ†       ‚ñ†   ‚ñ†   ‚ñ†   ‚ñ†\n",
    "‚ñ†           ‚ñ†   ‚ñ†   ‚ñ†       ‚ñ†\n",
    "‚ñ†       ‚ñ†   ‚ñ†       ‚ñ†       ‚ñ†\n",
    "‚ñ†   ‚ñ†   ‚ñ†       ‚ñ†   ‚ñ†   ‚ñ†   ‚óè\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True, map_name='8x8', render_mode='human')\n",
    "\n",
    "'''\n",
    "TODO\n",
    "actions = ???\n",
    "'''\n",
    "\n",
    "num_episodes = 10\n",
    "goals_reached = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    terminated = False\n",
    "    obs = actions[0]\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}')\n",
    "\n",
    "    while not terminated:\n",
    "        \n",
    "        # choose action basing on current observation\n",
    "        action = actions[obs]\n",
    "\n",
    "        # perform this action on the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if terminated and obs == 15:\n",
    "            '''\n",
    "            The observation is a value representing the agent's current position as\n",
    "            current_row * nrows + current_col (where both the row and col start at 0).\n",
    "            For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "            '''\n",
    "            \n",
    "            print('Success!')\n",
    "            goals_reached += 1\n",
    "            \n",
    "# end this instance of the environment\n",
    "env.close()\n",
    "print(f'\\nSuccess rate:\\t{goals_reached / num_episodes * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the map is random?\n",
    "As you see, any time the map changes, we have to rebuild our lookup table. \n",
    "\n",
    "- Is this a reliable approach?\n",
    "\n",
    "- How do we deal with random maps that change at each episode? \n",
    "\n",
    "- If the problem complexity increases, would we be able to create a valid heuristic policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Frozen Lake environment\n",
    "env = gym.make('FrozenLake-v1', desc=generate_random_map(size=8), is_slippery=True, render_mode='human')\n",
    "\n",
    "num_episodes = 10\n",
    "goals_reached = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    terminated = False\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}')\n",
    "\n",
    "    while not terminated:\n",
    "        \n",
    "        # sample a random action from the list of available actions\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # perform this action on the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if terminated and obs == 15:\n",
    "            '''\n",
    "            The observation is a value representing the agent's current position as\n",
    "            current_row * nrows + current_col (where both the row and col start at 0).\n",
    "            For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
    "            '''\n",
    "            \n",
    "            print('Success!')\n",
    "            goals_reached += 1\n",
    "            \n",
    "# end this instance of the environment\n",
    "env.close()\n",
    "print(f'\\nSuccess rate:\\t{goals_reached / num_episodes * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('AAgents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eb7827171240056e15aa4b7c2e3f34d100b23089e631116d38731716d346d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
