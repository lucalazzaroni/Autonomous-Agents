{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi\n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger’s location, picks up the passenger, drives to the passenger’s destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[1;32m     19\u001b[0m \u001b[39m# perform this action on the environment\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m terminated:\n\u001b[1;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSuccess!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Autonomous-Agents/gym-lib/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Autonomous-Agents/gym-lib/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Autonomous-Agents/gym-lib/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Autonomous-Agents/gym-lib/gym/envs/toy_text/taxi.py:262\u001b[0m, in \u001b[0;36mTaxiEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p, \u001b[39m\"\u001b[39m\u001b[39maction_mask\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_mask(s)})\n",
      "File \u001b[0;32m~/Autonomous-Agents/gym-lib/gym/envs/toy_text/taxi.py:290\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[0;32m~/Autonomous-Agents/gym-lib/gym/envs/toy_text/taxi.py:416\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    415\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 416\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    417\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    419\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    420\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# create Taxi environment\n",
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "\n",
    "num_episodes = 1\n",
    "goals_reached = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    terminated = False\n",
    "    print(f'Episode: {episode + 1}/{num_episodes}')\n",
    "\n",
    "    while not terminated:\n",
    "        \n",
    "        # sample a random action from the list of available actions\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # perform this action on the environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        \n",
    "        if terminated:\n",
    "            print('Success!')\n",
    "            goals_reached += 1\n",
    "            \n",
    "print(f'\\nSuccess rate:\\t{goals_reached / num_episodes * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "## Observations\n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "\n",
    "Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode. Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination. This gives a total of 404 reachable discrete states.\n",
    "\n",
    "Each state space is represented by the tuple: (taxi_row, taxi_col, passenger_location, destination)\n",
    "\n",
    "An observation is an integer that encodes the corresponding state. The state tuple can then be decoded with the “decode” method.\n",
    "### Passenger locations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "### Destinations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "# Observation decoding example\n",
    "taxi_row, taxi_col, pass_loc, dest_idx = env.decode(499)\n",
    "print(f'Row: {taxi_row}, Col: {taxi_col}, Passenger location: {pass_loc}, Destination index: {dest_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "- -1 per step unless other reward is triggered\n",
    "- +20 delivering passenger\n",
    "- -10 executing “pickup” and “drop-off” actions illegally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end this instance of the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gym_tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "477cec0dbe64c4c1ed98d3d857c925d7b4617f18ea089530c5910aa3207e1f15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
