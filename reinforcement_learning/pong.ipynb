{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pong\n",
    "Pong is played much like tennis. Using a Controller, each player rallys the ball by moving the paddles on the playfield.\n",
    "\n",
    "Paddles move only vertically on the playfield.\n",
    "\n",
    "A player scores one point when the opponent hits the ball out of bounds or misses a hit. The first player to score 21 points\n",
    "wins the game.\n",
    "\n",
    "The last player to score always serves the ball. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "You control the right paddle, you compete against the left paddle controlled by the computer. You each try to keep deflecting the ball away from your goal and into your opponent’s goal.\n",
    "## Actions\n",
    "- 0 NOOP\n",
    "- 1 FIRE\n",
    "- 2 RIGHT\n",
    "- 3 LEFT\n",
    "- 4 RIGHTFIRE\n",
    "- 5 LEFTFIRE\n",
    "\n",
    "Three of the six are redundant: FIRE is equal to NOOP, LEFT is equal to LEFTFIRE and RIGHT is equal to RIGHTFIRE.\n",
    "## Observations\n",
    "The environment returns the RGB image that is displayed to human players as an observation.\n",
    "## Rewards\n",
    "You get score points for getting the ball to pass the opponent’s paddle. You lose points if the ball passes your paddle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=16)\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.grid(False)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "Proximal Policy Optimization (PPO) is used in this example. The network architecure is a Convolutional Neural Network (CNN) which processes the RGB image of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('CnnPolicy', env, verbose=0, tensorboard_log='./tb_logging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-21.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(model, env, render=True, n_eval_episodes=5)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the agent for 1M steps, we can check results using the Tensorboard plugin.\n",
    "\n",
    "The model is then saved to be exploited later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=int(1e6))\n",
    "model.save('./pong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:19.60 +/- 1.02\n"
     ]
    }
   ],
   "source": [
    "# Trained Agent, after training\n",
    "mean_reward, std_reward = evaluate_policy(model, env, render=True, n_eval_episodes=5)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading\n",
    "A previously saved model can be loaded using the `load()` function so to continue its training or perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('./breakout')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gym_tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "477cec0dbe64c4c1ed98d3d857c925d7b4617f18ea089530c5910aa3207e1f15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
