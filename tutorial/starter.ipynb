{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter\n",
    "This notebook serves as a tutorial for OpenAI [Gym](https://www.gymlibrary.dev/). Gym is a standard Python API for Reinforcement Learning (RL), which includes a collection of pre-built environments and supports their representation, exploitation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first building block we encounter is the ```Env``` class. This implements a simulation of the environment we want to use. Since Gym has a lot of built-in environments, we'll start from one of them,  ```MountainCar```. Mountain Car is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Mountain Car environment and set the render mode to rgb_array so to have a picture of the environment available at each step\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created our environment, we have to understand how to interact with it and what we can extrapolate from it. The environment puts at our disposal two important attributes:\n",
    "- ```observation_space```: what we know about the environment at each timestep (e.g., our position, our speed, the target position, etc.)\n",
    "- ```action_space```: what we can perform inside the environment (e.g., turn left, turn right, accelerate, brake, etc.) \n",
    "\n",
    "The Mountain Car environment has an observation space with shape (2,), composed of the X-position of the Car and its velocity. The action space includes 3 possible actions: accelerate to the left, don't accelerate, accelerate to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(f'The observation space: {obs_space}') # Strucure: Low, High, shape, type\n",
    "print(f'The action space: {action_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the environment\n",
    "Two main functions allows the Agent to interact with the environment, ```reset``` and ```step```.\n",
    "\n",
    "- ```reset```: It resets the environment to its initial state. This is useful if the episode has terminated or something went wrong. It returns:\n",
    "    - ```observation```: The initial observation\n",
    "    - ```info```: Auxiliary diagnostic information (helpful for debugging, learning, and logging)   \n",
    "- ```step```: This function takes an action in input and execute it inside the environment. Afterwards, five values are returned:\n",
    "    - ```observation```: The new observation collected after the action execution\n",
    "    - ```reward```: A numerical value which is given to the Agent. An incentive mechanism that tells the agent what is correct and what is wrong, useful for RL purposes\n",
    "    - ```terminated```: Whether a _terminal state_ (as defined under the MDP of the task) is reached\n",
    "    - ```truncated```: Whether a truncation condition outside the scope of the MDP is satisfied. Typically a timelimit, but could also be used to indicate agent physically going out of bounds\n",
    "    - ```info```: Auxiliary diagnostic information (helpful for debugging, learning, and logging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment and see the initial observation\n",
    "obs, _ = env.reset()\n",
    "print(f'Current observation: {obs}')\n",
    "\n",
    "# Sample a random action from the entire action space\n",
    "random_action = env.action_space.sample()\n",
    "\n",
    "# # Take the action and get the new observation space\n",
    "new_obs, reward, terminated, truncated, info = env.step(random_action)\n",
    "print(f'New observation: {new_obs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_pic = env.render()\n",
    "plt.imshow(env_pic)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9eb7827171240056e15aa4b7c2e3f34d100b23089e631116d38731716d346d22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
